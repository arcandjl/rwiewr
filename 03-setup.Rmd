---
knit: "bookdown::preview_chapter"
---

```{r, echo=FALSE}
rm(list=ls())
```

# Setting up an evaluation

We describe our methods in this chapter.

## Statistical power, survey data and just doing it...

@Bloom1995
Optimal Design software: http://hlmsoft.net/od/

### Blocking

Consider a simple RCT regression ($i=1,...N$), with average treatment effect $\gamma$ and idiosyncractic treatment effects $\theta_{i}$, $E[\theta_{i}]=0$:
$$
Y_{i}=X_{i}\eta+D_{i}\gamma+D_{i}\theta_{i}+\epsilon_{i}
$$
Including covariates (*ex post*) or ``blocking" (*ex ante*), will reduce $\text{var}[U_{i}]$ and therefore improve precision:
$$
\text{var}[\widehat{\gamma}]=\frac{\text{var}[\epsilon_{i}]}{N\text{var}[D_{i}](1-R^{2}_{D|X})}=\frac{\text{var}[\epsilon_{i}]}{N\text{var}[D_{i}]}
$$

### Repeated observations

Add a time dimension $t$:
$$
Y_{it}=X_{it}\eta+D_{it}\gamma+D_{it}\theta_{i}+\lambda_{i}+\epsilon_{it}
$$
Gain in efficiency **as long as** $\theta_{i}\perp\lambda_{i}$ 
Standard panel data formula: 
$$
\text{var}[\widehat{\gamma}^{B}]-\text{var}[\widehat{\gamma}^{W}]=2N^{-1}\text{var}[\lambda_{i}]
$$
Disadvantage of within-subject designs: treatment effects may be time-dependent (history / learning):
$$
Y_{i2}=...+D_{i1}\gamma_{1}+D_{i2}\gamma_{2}+D_{i1}\theta_{i1}+D_{i2}\theta_{i2}...
$$

### Glossary and intuition


**Type I error** ($p-$ value or significance level): probability of falsely rejecting the null hypothesis

**Type II error**: probability of falsely not rejecting the null hypothesis = $1-$ power
 
**Power**: probability of correctly rejecting the null

**Effect size**: the magnitude of the treatment effect that you want to be able to detect

**Notation**: $Y_{ij}|X_{i}\sim N(\mu_{j},\sigma^{2}_{j}),j=0,1$; covariates $X_{i}$ have been "partialled out"

**Intuition**: $\text{H}_0:\mu_{0}=\mu_{1}\ +$  explicitly specify $\text{H}_1:\mu_{0}-\mu_{1}=\delta=$ **Minimum Detectable Effect Size (MDES)**

@Deaton97
@List2011
@Ranganathan2015

## Stuff to keep in mind:  Survey bias and Hawthorne effects

@Zwane2011
@Levitt2011

## Illustration: Was my research design underpowered?

A very inexpensive RCT project I set up a few years ago in Senegal led to the following amusing paper: @Arcand2010a. I just came across the total budget in my files: \$US 8,401.  There also is a published version of the paper that you might want to look at to get more context, but which (reasonably, I think) recodes some of the missing observations to zeroes (which is why the sample size is slightly higher):  @Wagner2014.  However, it uses an empirical specification I am no longer comfortable with, by adding combinations of quarter and region dummies to mop up residual variance ---I guess that I am getting more severe with myself as I age....  Be that as it may, the results are very similar.  The purpose of this exercise is to see (i) whether we were incredibly lucky in terms of the (lack of) power of our research design, and (ii) whether our results hold up to randomization inference. We got the power calculation wrong when we were planning the experiment, as you will see in the 2014 version of the paper.

Download the dataset: health_district1.dta. A short R file that will allow you to read in the data and appropriately define the outcomes and covariates is also provided: cnls1.R.   There are various R packages for computing optimal sample size and doing power calculation for Poisson regressions available online. You will find that the negative binomial and Poisson results are not very different, but go with the negbin if you are uneasy with overdispersion. The classic reference on power calculations for Poisson regression is @Signorini1991, as well as the more recent paper by @Channouf2014, for which R code is available at: http://neumann.hec.ca/pages/marc.fredette/Ajout\%20site\%20web.tx 

```{r, warning = FALSE, message = FALSE}
# Set up the data
options(digits = 4)

library(foreign)
library(lmtest)
library(readstata13)
library(reshape)
library(multiwayvcov)
cnls.data <- read.dta("health_district1.dta")
attach(cnls.data)

##################
# Recode dummies
##################

# Quarter dummies
cnls.data$d1 <- ifelse(periode == "01/2008",1,0) 
cnls.data$d2 <- ifelse(periode == "02/2008",1,0)
cnls.data$d3 <- ifelse(periode == "03/2008",1,0)
cnls.data$d4 <- ifelse(periode == "04/2008",1,0)
cnls.data$d5 <- ifelse(periode == "01/2009",1,0)

# Treatment status
# Control group is D1
cnls.data$D1 <- ifelse(treatment==1,1,0)
# Social mobilization is D2
cnls.data$D2 <- ifelse(treatment==2,1,0)
# Peer mentoring is D3
cnls.data$D3 <- ifelse(treatment==3,1,0)

# Combination of both treatments
cnls.data$D23 <- cnls.data$D2 + cnls.data$D3
cnls.data$num_hs <- factor(cnls.data$num_hs)

# Region dummies as a factor
cnls.data$num_reg <- factor(cnls.data$num_reg)

################################
# Define the response variables
# of choice --simply comment out
# the five others
################################

# Number of persons tested
cnls.data$y <- t

# Number of persons having 
# benefitted from pre-test counselling
#y <-c_m 

# Number of persons having picked up
# their test results
#y <-pick_m

# Number of persons who tested positive
# and who picked up their test results
#y <-pos_pick_m 

# Number of persons who tested positive
# and whose partner has been tested
#y <-partner_m

# Number of persons who tested positive
# and who have followed post-test counselling
#y <-pos_couns_m

#######################
# Create clean dataset
#######################
y_lm <- lm(y ~ D2 + D3 + D23 + d1 + d2 + d3 + d4 + d5 + num_hs + num_reg, data=cnls.data) 
clean <- model.frame(y_lm)
detach(cnls.data)
attach(clean) ; 
summary(clean)

###########################################
# Create histograms of response variable
# and notice that it is highly left-skewed
###########################################
hist(clean$y,main="",xlab="",br=150)


##############################
# Estimate Poisson regressions
##############################

y_glm <- glm(clean$y ~ clean$D2 + clean$D3 + clean$d1 + clean$d2
           + clean$d3 + clean$d4 + clean$num_reg, poisson(link = "log"))
summary(y_glm)
glm.vcovCL <- cluster.vcov(y_glm, clean$num_reg)
glm.clustered.se <- coeftest(y_glm, glm.vcovCL)
glm.clustered.se

#############################################
# Test for difference between two treatments
#############################################
y_glm <- glm(clean$y ~ clean$D3 + clean$D23 + clean$d1 + clean$d2
           + clean$d3 + clean$d4 + clean$num_reg, poisson(link = "log"))
glm.vcovCL <- cluster.vcov(y_glm, clean$num_reg)
glm.clustered.se <- coeftest(y_glm, glm.vcovCL)
glm.clustered.se

################################################################
# End of code
################################################################

# For perfect replicability of paper:
# Old code for calculating one-way clustered standard errors
# before the "multiwaycov" package was available
# Standard errors are almost identical
# See code by Mahmood Arai at http://people.su.se/~ma/clmclx.R

clx <- function(fm, dfcw, cluster) {
  library(sandwich)
  library(lmtest)
  M <- length(unique(cluster))
  N <- length(cluster)
  K <- fm$rank
  dfc <- (M/(M-1))*((N-1)/(N-K))
  u <- apply(estfun(fm),2,function(x) tapply(x,cluster,sum))
  vcovCL <- dfc*sandwich(fm,meat=crossprod(u)/N)*dfcw
  coeftest(fm,vcovCL)
}

clx(y_glm,1,clean$num_reg)
```

Carry out the correct power calculation that we should have done.
Subject our results to randomization inference.

